{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of our final model on benchmark RAID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import packages\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"torch.utils._pytree._register_pytree_node is deprecated\")\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from peft import PeftModel, PeftConfig\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disable parallel computing\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "# suppress warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\".*resume_download.*\", category=FutureWarning)\n",
    "os.environ[\"MKL_SERVICE_FORCE_INTEL\"] = \"1\"  # Suppresses Intel MKL warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current working directory\n",
    "current_dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import helper functions\n",
    "model_helpers_path = os.path.join(current_dir, \"src/model_helpers.py\")\n",
    "%run $model_helpers_path\n",
    "\n",
    "data_helpers_path = os.path.join(current_dir, \"src/data_helpers.py\")\n",
    "%run $data_helpers_path\n",
    "\n",
    "evaluate_helpers_path = os.path.join(current_dir, \"src/evaluate_helpers.py\")\n",
    "%run $evaluate_helpers_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted all files successfully.\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "data_dir = os.path.join(current_dir, \"data/evaluation/\")\n",
    "\n",
    "# Path to the zip file\n",
    "zip_file_path = os.path.join(data_dir, \"RAID_extra_noadv_df.csv.zip\")\n",
    "# Directory to extract to (can be the same as zip_file_path or another directory)\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "# Open the zip file\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    # Extract all contents into the specified directory\n",
    "    zip_ref.extractall(data_dir)\n",
    "\n",
    "print(f\"Extracted all files successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abb6799be1a54bc788c369c997c927f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ead56db3cc114e558cc355eb292e1ce2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b0be4551194408eab833adc143346c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'adv_source_id', 'source_id', 'model', 'decoding', 'repetition_penalty', 'attack', 'domain', 'title', 'prompt', 'generation'],\n",
      "    num_rows: 169925\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Load validation set without adversarial attacks\n",
    "validation_set = load_dataset(\"csv\", data_files=os.path.join(data_dir, \"RAID_extra_noadv_df.csv\"), split = 'all')\n",
    "print(validation_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7c8bab0a8274d929dee18e0345f096a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating labels column:   0%|          | 0/169925 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "277565382d1a4a0b979340ba6242509e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropping NAs:   0%|          | 0/169925 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d66ac29ad93462d800b86b19a1de50c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Transforming labels to binary 0/1:   0%|          | 0/169925 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Process the no-adversarial-attack validation set for training\n",
    "processed_set = process_data(validation_set, \"generation\", \"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ProcessedData with 169925 rows\n",
      "True\n",
      "Count of human-written entries: 4855\n",
      "Count of machine-generated entries: 165070\n"
     ]
    }
   ],
   "source": [
    "# Print summary of the processed validation set\n",
    "print(processed_set)\n",
    "print(is_processed_data(processed_set))\n",
    "label_counter(processed_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e88d80616b5463cbbbb5a08fca3ef9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filtering label 0:   0%|          | 0/169925 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2de2ef7e9d5246cf9fc5e05fa97738dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filtering label 1:   0%|          | 0/169925 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#! Remove this chunk and use the full dataset for evaluation\n",
    "\n",
    "# Choose a random subset of .1% entries without balanced labels\n",
    "#excluded_set, processed_set = train_test_split(validation_set, test_size=0.0001, seed=7)\n",
    "\n",
    "# Choose a random subset of 1% entries with balanced labels\n",
    "excluded_set, processed_set = train_test_split_equal_size(processed_set, test_size=0.01, seed=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ProcessedData with 96 rows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f91350222c654ab8a5bcaba7aa2d264b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropping NAs:   0%|          | 0/96 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ProcessedData with 96 rows\n"
     ]
    }
   ],
   "source": [
    "print(processed_set)\n",
    "\n",
    "# Remove columns other than text and labels\n",
    "reduced_processed_set = process_data(processed_set, \"text\", \"labels\", reduced=True)\n",
    "print(reduced_processed_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the model directory\n",
    "model_dir = os.path.join(current_dir, \"ML-LoRA-E5/mix_data/final_model\")\n",
    "\n",
    "# Get the directory of the latest model checkpoints\n",
    "lora_checkpoints_dir = os.path.join(current_dir, \"ML-LoRA-E5/mix_data/results_LoRA_e5/checkpoint-53390\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at intfloat/e5-small and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer and base model (e.g., BERT, GPT-2)\n",
    "base_model_name = \"intfloat/e5-small\"  # Change to the base model you used\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(base_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the LoRA fine-tuned model (with the latest checkpoints)\n",
    "our_model = PeftModel.from_pretrained(base_model, lora_checkpoints_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "tensor([[-2.1088,  2.0489]])\n"
     ]
    }
   ],
   "source": [
    "# Sample text for inference\n",
    "text = \"This is an example text to test the model.\"\n",
    "\n",
    "# Tokenize the input\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# Run inference\n",
    "with torch.no_grad():\n",
    "    outputs = our_model(**inputs)\n",
    "\n",
    "# Access model logits or predictions\n",
    "logits = outputs.logits\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate our detector on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bb5ec0c05e84e80a94da710478f8c86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing data:   0%|          | 0/96 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predicted_probs = inference_model(our_model, reduced_processed_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined DataFrame saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Save predictions to a CSV file\n",
    "res_csv_path = os.path.join(data_dir, 'pred_extra_nonadv_subset.csv')\n",
    "save_inference_to_csv(predicted_probs, processed_set, res_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "{'Accuracy': 0.583, 'F1 score': 0.583, 'FPR': 0.417}\n"
     ]
    }
   ],
   "source": [
    "print(isinstance(predicted_probs, PredictionResults))\n",
    "\n",
    "# Compute metrics\n",
    "predicted_label = get_predicted_labels(predicted_probs, 0.9)\n",
    "res = evaluate_model(predicted_label, reduced_processed_set)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     id                         adv_source_id  \\\n",
      "0  ed337d3f-b9d7-4980-b60c-a1961f73bd05  ed337d3f-b9d7-4980-b60c-a1961f73bd05   \n",
      "\n",
      "                              source_id model  decoding repetition_penalty  \\\n",
      "0  5795208a-5b5c-456d-a137-994297c76d03  gpt4  sampling                 no   \n",
      "\n",
      "  attack  domain                                          title  \\\n",
      "0   none  german  Barcelona nur 2:2 in Villarreal, Real gewinnt   \n",
      "\n",
      "                                              prompt  \\\n",
      "0  Schreiben Sie einen Nachrichtenartikel mit dem...   \n",
      "\n",
      "                                                text  labels  Prediction_0  \\\n",
      "0  In der spanischen Fußball-Liga Primera Divisió...       1      0.038074   \n",
      "\n",
      "   Prediction_1  \n",
      "0      0.961926  \n"
     ]
    }
   ],
   "source": [
    "# Read the results as pandas.DataFrame\n",
    "res_dataframe = read_inference_as_DataFrame(res_csv_path)\n",
    "print(res_dataframe.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': ['ed337d3f-b9d7-4980-b60c-a1961f73bd05'], 'adv_source_id': ['ed337d3f-b9d7-4980-b60c-a1961f73bd05'], 'source_id': ['5795208a-5b5c-456d-a137-994297c76d03'], 'model': ['gpt4'], 'decoding': ['sampling'], 'repetition_penalty': ['no'], 'attack': ['none'], 'domain': ['german'], 'title': ['Barcelona nur 2:2 in Villarreal, Real gewinnt'], 'prompt': ['Schreiben Sie einen Nachrichtenartikel mit dem Titel \"Barcelona nur 2:2 in Villarreal, Real gewinnt\".'], 'text': ['In der spanischen Fußball-Liga Primera División hat der Rennleiter FC Barcelona nur ein Unentschieden gegen den CF Villarreal erreicht. Damit hat Barcelona trotz starker Bemühungen wichtige Punkte verloren und ist jetzt auf dem dritten Platz in der Tabelle gelandet. \\n\\nDas Spiel in Villarreal endete mit einem 2:2. Barcelona hatte die Führung in der ersten Spielhälfte mit Toren von Messi und Griezmann übernommen. In der zweiten Spielhälfte kämpfte Villarreal jedoch zurück und erzielte in den letzten Minuten zwei Tore, was zu einem Unentschieden führte.\\n\\nDer CF Villarreal zeigte seine Heimstärke und sorgte damit für eine große Überraschung. Besonders das Umschaltspiel von Villarreal und der unermüdliche Kampfgeist der Mannschaft beeindruckten die Fans. \\n\\nIn Madrid wendete sich das Blatt zur Freude von Real. Die Königlichen feierten einen späten 3:2-Sieg gegen den FC Sevilla. Dieser Sieg verhalf Real Madrid, die Tabellenspitze zu behaupten.\\n\\nDas Unentschieden von Barcelona und der Sieg von Real Madrid brachten eine bedeutende Veränderung in der Tabelle. Real Madrid steht jetzt mit 42 Punkten an der Spitze, gefolgt von dem FC Barcelona mit 40 Punkten. \\n\\nDer Kampf um die Meisterschaft ist damit weiterhin offen und die restlichen Spieltage versprechen Spannung und Nervenkitzel. Es bleibt also abzuwarten, wie sich die Dinge in den nächsten Wochen entwickeln.'], 'labels': [1], 'Prediction_0': [0.0380738], 'Prediction_1': [0.96192616]}\n"
     ]
    }
   ],
   "source": [
    "# Read the results as datasets.Dataset\n",
    "res_dataset = read_inference_as_Dataset(res_csv_path)\n",
    "print(res_dataset[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
