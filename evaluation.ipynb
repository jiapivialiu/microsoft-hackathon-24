{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of our final model on benchmark RAID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import packages\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"torch.utils._pytree._register_pytree_node is deprecated\")\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from peft import PeftModel, PeftConfig\n",
    "from datasets import load_dataset\n",
    "# import RAID\n",
    "import json\n",
    "from raid import run_detection, run_evaluation\n",
    "from raid.utils import load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disable parallel computing\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "# suppress warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\".*resume_download.*\", category=FutureWarning)\n",
    "os.environ[\"MKL_SERVICE_FORCE_INTEL\"] = \"0\"  # Suppresses Intel MKL warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the working directories\n",
    "current_dir = os.getcwd()\n",
    "model_dir = os.path.join(current_dir, \"ML-LoRA-E5/mix_data/\")\n",
    "data_dir = os.path.join(current_dir, \"data/\")\n",
    "eval_dir = os.path.join(data_dir, \"evaluation/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import helper functions\n",
    "model_helpers_path = os.path.join(current_dir, \"src/model_helpers.py\")\n",
    "%run $model_helpers_path\n",
    "\n",
    "data_helpers_path = os.path.join(current_dir, \"src/data_helpers.py\")\n",
    "%run $data_helpers_path\n",
    "\n",
    "evaluate_helpers_path = os.path.join(current_dir, \"src/evaluate_helpers.py\")\n",
    "%run $evaluate_helpers_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the directory of the latest model checkpoints\n",
    "lora_checkpoints_dir = os.path.join(model_dir, \"results_LoRA_e5/checkpoint-53390\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at intfloat/e5-small and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer and base model (e.g., BERT, GPT-2)\n",
    "base_model_name = \"intfloat/e5-small\"  # Change to the base model you used\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(base_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the LoRA fine-tuned model (with the latest checkpoints)\n",
    "our_model = PeftModel.from_pretrained(base_model, lora_checkpoints_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.1088,  2.0489]])\n"
     ]
    }
   ],
   "source": [
    "# Sample text for inference\n",
    "text = \"This is an example text to test the model.\"\n",
    "\n",
    "# Tokenize the input\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# Run inference\n",
    "with torch.no_grad():\n",
    "    outputs = our_model(**inputs)\n",
    "\n",
    "# Access model logits or predictions\n",
    "logits = outputs.logits\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Evaluate our detector on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Load the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Evaluate the detector on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Evaluate our detector on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the RAID test data\n",
    "test_df = load_data(split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          id  \\\n",
      "0       64005577-3d63-4583-8945-7541d3e53e7d   \n",
      "1       c2b9df67-4e29-45ca-bdcc-7065fb907b77   \n",
      "2       07904f22-8530-4d3b-bf49-6bd1642d89f7   \n",
      "3       dc5aa023-6f57-4f9c-833a-c0f322a994fa   \n",
      "4       1b1ab19b-fe6f-458d-a666-06bbc1791534   \n",
      "...                                      ...   \n",
      "671995  b2694dd7-1c4d-4bef-8e52-0c1e13d54130   \n",
      "671996  9e5c1a37-9305-4ca7-8dc0-ab1ed763231e   \n",
      "671997  a233aa5d-b375-423f-ad7a-ffc5045398c5   \n",
      "671998  e267ebb4-b1f7-4af4-b68b-ecd4ba565f93   \n",
      "671999  45df9738-b31c-495b-9a0d-2c62220df990   \n",
      "\n",
      "                                               generation  \n",
      "0         The Sunspot Number, created by R.Wolf in 184...  \n",
      "1         We present several analogies between convex ...  \n",
      "2         Let H be a homology theory for algebraic var...  \n",
      "3         The two parallel concepts of \"small\" sets of...  \n",
      "4         We present new solutions to the strong explo...  \n",
      "...                                                   ...  \n",
      "671995  R​o​n​n​i​e​ ​N​u​n​n​ ​i​s​ ​a​n​ ​A​m​e​r​i​...  \n",
      "671996  L​e​s​t​e​r​ ​M​.​ ​W​o​l​f​s​o​n​ ​w​a​s​ ​a​...  \n",
      "671997  L​e​s​t​e​r​ ​M​.​ ​W​o​l​f​s​o​n​ ​w​a​s​ ​a​...  \n",
      "671998  J​u​s​t​i​n​i​a​n​ ​R​w​e​y​e​m​a​m​u​ ​(​b​o​...  \n",
      "671999  J​u​s​t​i​n​i​a​n​ ​R​w​e​y​e​m​a​m​u​ ​(​b​o​...  \n",
      "\n",
      "[672000 rows x 2 columns]\n",
      "True\n",
      "Dataset({\n",
      "    features: ['id', 'generation'],\n",
      "    num_rows: 672000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(test_df)\n",
    "\n",
    "test_ds = Dataset.from_pandas(test_df)\n",
    "print(isinstance(test_ds, Dataset))\n",
    "print(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     id  \\\n",
      "0  c39744bc-bfb4-49b4-8508-4149547be2a5   \n",
      "1  9c5acac0-def5-44d5-a154-9863a6eab11b   \n",
      "2  7b8b7930-8aee-4be5-8208-ff41e742d669   \n",
      "\n",
      "                                          generation  \n",
      "0  Ingredients:\\n- 1 loaf of corn bread, crumbled...  \n",
      "1  Feker Libi, is a song  by Ehud Banai  and Yeho...  \n",
      "2  Rebecca L. Gottesman (born 1962) is an America...  \n"
     ]
    }
   ],
   "source": [
    "# Generate a random subset for testing\n",
    "test_subset = test_ds.train_test_split(test_size=0.001, shuffle=True, seed=12)['test']\n",
    "test_subset_df = Dataset.to_pandas(test_subset)\n",
    "print(test_subset_df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define my detector function for evaluation on RAID test set\n",
    "def my_detector(texts: list[str], model = our_model) -> list[float]:\n",
    "    \n",
    "    #processed_data = process_data(texts)\n",
    "    #predicted_prob = inference_model(model, processed_ata)\n",
    "\n",
    "    data = Dataset.from_dict({\"text\": texts})\n",
    "    \n",
    "    # tokenize input data\n",
    "    data = data.map(lambda x: tokenize_data(x), batched=True, desc=\"Tokenizing data\")\n",
    "    # load batches of tokenized data\n",
    "    dataloader = DataLoader(data, batch_size=1, collate_fn=custom_collate_fn, shuffle=False)\n",
    "    # create a list for the output prediction\n",
    "    predictions = []\n",
    "    # Counter for entries processed\n",
    "    entry_count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            batch = {k: v.to(model.device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            logits = outputs.logits\n",
    "            # Apply softmax to get probabilities\n",
    "            probs = softmax(logits, dim=-1).cpu().numpy()\n",
    "            predictions.extend(probs)\n",
    "            \n",
    "            # Update counter and show progress every 100 entries\n",
    "            entry_count += len(probs)\n",
    "            if entry_count % 100 == 0:\n",
    "                print(f\"Processed {entry_count} entries...\")\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9510f66b52e40b69a2d2c1b3b543941",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing data:   0%|          | 0/672 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 entries...\n",
      "Processed 200 entries...\n",
      "Processed 300 entries...\n",
      "Processed 400 entries...\n",
      "Processed 500 entries...\n",
      "Processed 600 entries...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'predictions.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Run your detector on the dataset\u001b[39;00m\n\u001b[1;32m      2\u001b[0m predictions \u001b[38;5;241m=\u001b[39m run_detection(my_detector, test_subset_df)\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpredictions.json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      5\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump(predictions, f)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.10/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'predictions.json'"
     ]
    }
   ],
   "source": [
    "# Run your detector on the dataset\n",
    "predictions = run_detection(my_detector, test_subset_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'c39744bc-bfb4-49b4-8508-4149547be2a5', 'score': [0.03664873167872429, 0.963351309299469]}\n"
     ]
    }
   ],
   "source": [
    "predictions_dicts = [{\"id\": pred['id'], \"score\": pred['score']} for pred in predictions]\n",
    "for pred in predictions_dicts:\n",
    "    pred['score'] = pred['score'].tolist()  # Convert the NumPy array to a list\n",
    "\n",
    "print(predictions_dicts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(data_dir, \"predictions.json\"), 'w') as f:\n",
    "    json.dump(predictions_dicts, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluate our detector on an extra set with new domain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Load the extra set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted all files successfully.\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "\n",
    "# Path to the zip file\n",
    "zip_file_path = os.path.join(data_dir, \"RAID_extra_noadv_df.csv.zip\")\n",
    "# Directory to extract to (can be the same as zip_file_path or another directory)\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "# Open the zip file\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    # Extract all contents into the specified directory\n",
    "    zip_ref.extractall(data_dir)\n",
    "\n",
    "print(f\"Extracted all files successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abb6799be1a54bc788c369c997c927f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ead56db3cc114e558cc355eb292e1ce2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b0be4551194408eab833adc143346c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'adv_source_id', 'source_id', 'model', 'decoding', 'repetition_penalty', 'attack', 'domain', 'title', 'prompt', 'generation'],\n",
      "    num_rows: 169925\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Load validation set without adversarial attacks\n",
    "validation_set = load_dataset(\"csv\", data_files=os.path.join(eval_dir, \"RAID_extra_noadv_df.csv\"), split = 'all')\n",
    "print(validation_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7c8bab0a8274d929dee18e0345f096a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating labels column:   0%|          | 0/169925 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "277565382d1a4a0b979340ba6242509e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropping NAs:   0%|          | 0/169925 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d66ac29ad93462d800b86b19a1de50c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Transforming labels to binary 0/1:   0%|          | 0/169925 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Process the no-adversarial-attack validation set for training\n",
    "processed_set = process_data(validation_set, \"generation\", \"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ProcessedData with 169925 rows\n",
      "True\n",
      "Count of human-written entries: 4855\n",
      "Count of machine-generated entries: 165070\n"
     ]
    }
   ],
   "source": [
    "# Print summary of the processed validation set\n",
    "print(processed_set)\n",
    "print(is_processed_data(processed_set))\n",
    "label_counter(processed_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e88d80616b5463cbbbb5a08fca3ef9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filtering label 0:   0%|          | 0/169925 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2de2ef7e9d5246cf9fc5e05fa97738dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filtering label 1:   0%|          | 0/169925 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#! Remove this chunk and use the full dataset for evaluation\n",
    "\n",
    "# Choose a random subset of .1% entries without balanced labels\n",
    "#excluded_set, processed_set = train_test_split(validation_set, test_size=0.0001, seed=7)\n",
    "\n",
    "# Choose a random subset of 1% entries with balanced labels\n",
    "excluded_set, processed_set = train_test_split_equal_size(processed_set, test_size=0.01, seed=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ProcessedData with 96 rows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f91350222c654ab8a5bcaba7aa2d264b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropping NAs:   0%|          | 0/96 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ProcessedData with 96 rows\n"
     ]
    }
   ],
   "source": [
    "print(processed_set)\n",
    "\n",
    "# Remove columns other than text and labels\n",
    "reduced_processed_set = process_data(processed_set, \"text\", \"labels\", reduced=True)\n",
    "print(reduced_processed_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Evaluate our detector on the extra set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bb5ec0c05e84e80a94da710478f8c86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing data:   0%|          | 0/96 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predicted_probs = inference_model(our_model, reduced_processed_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined DataFrame saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Save predictions to a CSV file\n",
    "res_csv_path = os.path.join(eval_dir, 'pred_extra_nonadv_subset.csv')\n",
    "save_inference_to_csv(predicted_probs, processed_set, res_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "{'Accuracy': 0.583, 'F1 score': 0.583, 'FPR': 0.417}\n"
     ]
    }
   ],
   "source": [
    "print(isinstance(predicted_probs, PredictionResults))\n",
    "\n",
    "# Compute metrics\n",
    "predicted_label = get_predicted_labels(predicted_probs, 0.9)\n",
    "res = evaluate_model(predicted_label, reduced_processed_set)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     id                         adv_source_id  \\\n",
      "0  ed337d3f-b9d7-4980-b60c-a1961f73bd05  ed337d3f-b9d7-4980-b60c-a1961f73bd05   \n",
      "\n",
      "                              source_id model  decoding repetition_penalty  \\\n",
      "0  5795208a-5b5c-456d-a137-994297c76d03  gpt4  sampling                 no   \n",
      "\n",
      "  attack  domain                                          title  \\\n",
      "0   none  german  Barcelona nur 2:2 in Villarreal, Real gewinnt   \n",
      "\n",
      "                                              prompt  \\\n",
      "0  Schreiben Sie einen Nachrichtenartikel mit dem...   \n",
      "\n",
      "                                                text  labels  Prediction_0  \\\n",
      "0  In der spanischen Fußball-Liga Primera Divisió...       1      0.038074   \n",
      "\n",
      "   Prediction_1  \n",
      "0      0.961926  \n"
     ]
    }
   ],
   "source": [
    "# Read the results as pandas.DataFrame\n",
    "res_dataframe = read_inference_as_DataFrame(res_csv_path)\n",
    "print(res_dataframe.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': ['ed337d3f-b9d7-4980-b60c-a1961f73bd05'], 'adv_source_id': ['ed337d3f-b9d7-4980-b60c-a1961f73bd05'], 'source_id': ['5795208a-5b5c-456d-a137-994297c76d03'], 'model': ['gpt4'], 'decoding': ['sampling'], 'repetition_penalty': ['no'], 'attack': ['none'], 'domain': ['german'], 'title': ['Barcelona nur 2:2 in Villarreal, Real gewinnt'], 'prompt': ['Schreiben Sie einen Nachrichtenartikel mit dem Titel \"Barcelona nur 2:2 in Villarreal, Real gewinnt\".'], 'text': ['In der spanischen Fußball-Liga Primera División hat der Rennleiter FC Barcelona nur ein Unentschieden gegen den CF Villarreal erreicht. Damit hat Barcelona trotz starker Bemühungen wichtige Punkte verloren und ist jetzt auf dem dritten Platz in der Tabelle gelandet. \\n\\nDas Spiel in Villarreal endete mit einem 2:2. Barcelona hatte die Führung in der ersten Spielhälfte mit Toren von Messi und Griezmann übernommen. In der zweiten Spielhälfte kämpfte Villarreal jedoch zurück und erzielte in den letzten Minuten zwei Tore, was zu einem Unentschieden führte.\\n\\nDer CF Villarreal zeigte seine Heimstärke und sorgte damit für eine große Überraschung. Besonders das Umschaltspiel von Villarreal und der unermüdliche Kampfgeist der Mannschaft beeindruckten die Fans. \\n\\nIn Madrid wendete sich das Blatt zur Freude von Real. Die Königlichen feierten einen späten 3:2-Sieg gegen den FC Sevilla. Dieser Sieg verhalf Real Madrid, die Tabellenspitze zu behaupten.\\n\\nDas Unentschieden von Barcelona und der Sieg von Real Madrid brachten eine bedeutende Veränderung in der Tabelle. Real Madrid steht jetzt mit 42 Punkten an der Spitze, gefolgt von dem FC Barcelona mit 40 Punkten. \\n\\nDer Kampf um die Meisterschaft ist damit weiterhin offen und die restlichen Spieltage versprechen Spannung und Nervenkitzel. Es bleibt also abzuwarten, wie sich die Dinge in den nächsten Wochen entwickeln.'], 'labels': [1], 'Prediction_0': [0.0380738], 'Prediction_1': [0.96192616]}\n"
     ]
    }
   ],
   "source": [
    "# Read the results as datasets.Dataset\n",
    "res_dataset = read_inference_as_Dataset(res_csv_path)\n",
    "print(res_dataset[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
